{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56DLise0K_24",
        "outputId": "b7c0fa3d-4d11-4706-b4e1-1b982de635a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=f4ea6ac29fb56161d6b06ab3d8e786bf3d37ce649007a5d8c90efcc878146c7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Debido a la dificultad que nos encontramos al intentar realizar un ETL Job desde glue sobre los archivos JSON en el S3, decidimos procesarlos antes de subirlos al S3 para poder sobrellevar esta dificultad, esto lo hicimos con el siguiente código que permite transformar los archivos JSON a archivos PARQUET que nos permite trabajar con ellos de manera más sencilla como los necesitamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-rDISsPKYCU",
        "outputId": "a0bd529c-8bff-4960-fc30-3aee1e08c11a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explode, col\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, col\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Procesar SIATA JSON\").getOrCreate()\n",
        "\n",
        "\n",
        "input_path = \"./siata/raw/\"\n",
        "output_path = \"./siata/processed/\"\n",
        "\n",
        "\n",
        "json_files = [\"co\", \"no\", \"so2\", \"no2\", \"ozono\", \"pm1\", \"pm10\", \"pm25\"]\n",
        "\n",
        "for file_name in json_files:\n",
        "\n",
        "    df = spark.read.json(f\"{input_path}/Datos_SIATA_Aire_{file_name}.json\")\n",
        "\n",
        "\n",
        "    df = df.withColumn(\"datos\", explode(col(\"datos\")))\n",
        "\n",
        "    df = df.select(\n",
        "        col(\"latitud\"),\n",
        "        col(\"codigoSerial\"),\n",
        "        col(\"datos.variableConsulta\"),\n",
        "        col(\"datos.fecha\"),\n",
        "        col(\"datos.calidad\"),\n",
        "        col(\"datos.valor\"),\n",
        "        col(\"nombre\"),\n",
        "        col(\"nombreCorto\"),\n",
        "        col(\"longitud\")\n",
        "    )\n",
        "\n",
        "    df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{output_path}{file_name}_processed.parquet\")\n",
        "\n",
        "    print(f\"Archivo {file_name} procesado y guardado en {output_path}\")\n",
        "\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
